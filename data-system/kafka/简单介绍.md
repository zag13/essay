# Kafka

## 基本概念

- 主题（Topic）
    - kafka 将一组消息抽象为一个主题
- 消息（Record）
    - kafka 通信的基本单位，由一个固定长度的消息头和一个可变长度的消息体构成。
- 分区和副本（Partition、Replica）
    - 每个主题被分成一个或多个分区，每个分区又有一至多个副本
    - 每个分区由一系列有序、不可变的消息组成，是一个有序队列
    - 分区的副本分布在集群的不同代理上
- Leader 副本和 Follower 副本
- 偏移量
    - 任何发布到分区的消息都会被追加进日志文件的尾部，消息在该日志文件中的位置就会对应一个按序增加的偏移量
- 日志段（LogSegment）
    - 一个日志又被划分为多个日志段，日志段是 kafka 日志对象分片的最小单位
    - 一个日志段对应磁盘上的一个具体日志文件（.log）和两个索引文件（.index、.timeindex）
- 代理（Broker）
    - kafka 集群是由一个或多个 kafka 实例构成，其中的 kafka 实例被称为代理
- 生产者
    - 向 kafka 代理发送消息的客户端
- 消费者和消费组
    - 消费的客户端
    - 每一个消费者都属于一个特定消费组
    - 同一主题下的一个消息只能被同一个消费组下某一个消费者消费，但不同消费组的消费者可以同时消费该消息
- ISR（In-sync Replica）
    - 保存同步的副本列表
- ZooKeeper
    - kafka 使用 zookeeper 保存相应元数据
    - kafka3.0 开始逐步测试使用 KRaft 代替 zookeeper

## CONFIGURATION

### Broker Configs

```
# broker的全局唯一编号，不能重复
broker.id=0
 
# 侦听器列表
listeners=PLAINTEXT://:9092
 
# 侦听器名称和安全协议之间的映射
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# 发布到 ZooKeeper 以供客户端使用的侦听器
advertised.listeners=null

# broker 需要使用 zookeeper 保存元数据
zookeeper.connect=zk-01:2181,zk-02:2181,zk-03:2181

# kafka 存放日志的路径
log.dirs=/tmp/kafka-logs

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown
num.recovery.threads.per.data.dir=1

# 允许服务器自动创建 topic
auto.create.topics.enable=true

# topic 在当前 broker 上的分片个数
num.partitions=2

# 删除前保留日志文件的小时数
log.retention.hours=168
 
# 删除前日志的最大大小
log.retention.bytes=-1
 
# 日志文件中每个 segment 的大小，默认为1G
log.segment.bytes=1073741824
 
# 滚动生成新的 segment 文件的最大时间
log.roll.hours=168
```

### Producer Configs

```
# broker 的地址清单
bootstrap.servers=kafka-01:9092

key.serializer=

value.serializer=

# 用来标记生产者
client.id=

# 0: 不在乎是否写入成功； 1: 写入 leader 成功； all: 写入 leader 和所有副本都成功；
acks=all

# 生产者内存缓冲区的大小 
buffer.memory=33554432

# 消息的压缩算法
compression.type=none

# 生产者可以重发消息的次数
retries=3

# 多个消息发送到同一个分区时，生产者会把他们放到同一个批次
# 该参数指定批次可使用内存的大小
batch.size=16384
 
# 一个批次的最大等待时间
linger.ms=120000

# 生产者在阻塞之前单个连接上发送的未确认请求的最大数量
max.in.flight.requests.per.connection=5

# 生产者等待请求响应的最长时间
request.timeout.ms=30000

# 在调用 send()、partitionsFor() 获取元数据时生产者的阻塞时间
max.block.ms=60000

# 请求的最大大小（以字节为单位）
max.request.size=1048576

# 读取数据时使用的 TCP 接收缓冲区 (SO_RCVBUF) 的大小
receive.buffer.bytes=65536

# 发送数据时使用的 TCP 发送缓冲区 (SO_SNDBUF) 的大小
send.buffer.bytes=131072
```

### Consumer Configs

```
# broker 的地址清单
bootstrap.servers=kafka-01:9092

key.deserializer=

value.deserializer=

# 用来标记消费组
group.id=

client.id=

# 一次拉取操作等待消息的最小字节数
fetch.min.bytes=1

# 一次拉取操作获取消息的最大字节数
fetch.max.bytes=57671680

# 若是不满足 fetch.min.bytes 时，客户端等待请求的最长等待时间
fetch.wait.max.ms=500

# 服务器从每个分区里返回给消费者的最大字节数
max.partition.fetch.bytes=1048576

# 认定消费组故障的时间
session.timeout.ms=10000

# 没有偏移量或偏移量失效时该怎么读取
auto.offset.reset=latest

# 是否开启自动提交消费偏移量
enable.auto.commit=true

# 自动提交消费偏移量的时间间隔
auto.commit.interval.ms=5000

# 一次拉取消息的最大数量
max.poll.records=500

# 使用消费组管理消费者时，消费者超过这个时间还没有 poll 动作，
# 消费组会认为消费者已离开消费组，触发平衡操作
max.poll.interval.ms=3000000
 
# 读取数据时使用的 TCP 接收缓冲区 (SO_RCVBUF) 的大小
receive.buffer.bytes=65536

# 发送数据时使用的 TCP 发送缓冲区 (SO_SNDBUF) 的大小
send.buffer.bytes=131072
```
